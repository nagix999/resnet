{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiBBG1xVkd0u",
        "outputId": "fef2c086-9180-4f7c-84d4-1389de0a8976"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.11\n",
            "\"---------------------------------------------\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'cat'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
            "��ġ ������ �ƴմϴ�.\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!pip list > requirements.txt\n",
        "!echo \"---------------------------------------------\"\n",
        "!cat requirements.txt | grep \"torch\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia2wGSK1aG20"
      },
      "source": [
        "# Deep Residual Learning for Image Recognition\n",
        "\n",
        "- 논문URL - https://arxiv.org/abs/1512.03385v1\n",
        "\n",
        "- 핵심 내용\n",
        "    - 신경망이 더 깊어질수록 성능 저하(degradation problem)가 발생하는 문제가 발생(정확도의 감소, 반대로 training error와 test error는 증가)\n",
        "        - 이러한 성능 저하는 과적합에 의한 것이 아님\n",
        "        - 또한, 기울기 소실이나 폭주(Gradient Vanishing/Exploding)의 문제가 아니라, 깊은 신경망에서 옵티마이저가 겪는 최적화 문제임\n",
        "\n",
        "    - 심층 잔여 학습 프레임워크(Deep Residual Learning Framework) 도입을 제안\n",
        "        - F(x) + x 공식으로 나타나는 shortcut connections 를 통해 입력을 해당 계층의 출력에 덧셈 연산을 통해 수행\n",
        "        - 전체 네트워크는 SGD 역전파를 통해 End-To-End 방식으로 훈련할 수 있음\n",
        "        - Solver(optimizer)를 수정하지 않고도 Caffe와 같은 일반적인 라이브러리에서 쉽게 구현할 수 있음\n",
        "    \n",
        "    - ImageNet 데이터를 학습한 결과, 기본 네트워크(Residual이 적용되지 않은) 18-layer와 50-layer를 비교하였을 때 50-layer에서 성능이 저하되어 18-layer보다 낮은 성능을 보였으나, Residual Learning이 적용된 네트워크는 18-layer보다 50-layer가 더 높은 성능을 보이며 수렴 속도도 더 빠름\n",
        "        - 34 계층 Resnet은 매우 경쟁력 있는 정확도를 달성했고, 상위 5개의 검증 오차는 4.49%임\n",
        "        - 152 계층 모델 2개를 결합한 앙상블을 구성하여 ILSVRC 2015에서 1위 입상"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JcyNY8D-M1r"
      },
      "source": [
        "## Residual Learing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24BkJnBK9v58"
      },
      "source": [
        "<img src=\"../images/2-Figure2-1.png\" alt=\"Figure 2\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEe8mh12-dtT"
      },
      "source": [
        "### Basic Block & Bottleneck Block\n",
        "컨볼루션 잔여 블록의 두 가지 변형\n",
        "- 왼쪽: 3x3 컨볼루션 레이어 2개가 있는 기본 블록\n",
        "- 오른쪽: 차원 감소(예: 1/4)를 위한 1x1 컨볼루션 레이어, 3x3 컨볼루션 레이어, 차원 복원을 위한 또 다른 1x1 컨볼루션 레이어가 있는 병목 블록"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsxkD1S6-bGV"
      },
      "source": [
        "<img src=\"../images/6-Figure5-1.png\" alt=\"Figure 5\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zTqIJdhyUbQ"
      },
      "source": [
        "### Basic Block 간단 구현\n",
        "\n",
        "Residual Block 중 하나인 Basic Block을 구현해본다.\n",
        "\n",
        "블록을 통과한 출력과 입력을 합하기 위해서는 출력과 입력이 동일한 차원이어야 한다. 따라서 두 합성곱 계층(3 x 3, 64)을 통과하면서 피쳐맵 크기가 변하지 않아야 하므로 padding을 1로 설정한다. 논문 내용에 따라 합성곱 계층 뒤에는 배치 정규화를 위치시킨다.\n",
        "\n",
        "`nn.Sequential`을 사용해 합성곱, 배치 정규화, 활성화 함수를 묶고 이 계층들을 통과한 출력에 입력을 합산하여 최종 출력으로 산출한다. `print`를 통해 입력과 출력 모두 동일한 차원을 갖는 것을 확인할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "np-YXONUsB2E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixzaA3ze-KX3",
        "outputId": "63a763f0-4bb7-4fe9-f6cc-b28cdfbdceb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input shape: torch.Size([1, 64, 56, 56])\n",
            "output shape: torch.Size([1, 64, 56, 56])\n"
          ]
        }
      ],
      "source": [
        "dim = 64\n",
        "input = torch.rand(1, 64, 56, 56)\n",
        "\n",
        "# Basic Block\n",
        "layers = nn.Sequential(\n",
        "    nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1),\n",
        "    nn.BatchNorm2d(dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1),\n",
        "    nn.BatchNorm2d(dim)\n",
        ")\n",
        "\n",
        "out = layers(input) + input\n",
        "out = torch.relu(out)\n",
        "print(f\"input shape: {input.shape}\")\n",
        "print(f\"output shape: {out.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hn5X5UBLBebw"
      },
      "source": [
        "#### 피처맵 차원 변화에 따른 다운 샘플링 적용\n",
        "\n",
        "피처맵 크기를 줄임으로써 연산량은 감소하게 된다. Basic Block에서도 첫번째 합성곱 계층에서 stride를 2로 설정하여 피처맵 크기를 절반으로 감소시키는 경우가 있다.\n",
        "이 경우 입력과 합성곱 블록을 통과한 출력의 차원은 동일하지 않으므로, 입력에 다운 샘플링을 적용하여 출력 차원과 동일하게 변환한 후 덧셈 연산을 수행한다. 다운 샘플링은 kernel size 1, stride 2인 합성곱 연산을 수행하여 피처맵 크기를 절반으로 감소시킨다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXI8d3rEAINB",
        "outputId": "ab571169-53be-4afd-e784-e969d7d52c76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input shape: torch.Size([1, 64, 56, 56])\n",
            "output shape: torch.Size([1, 64, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "dim = 64\n",
        "input = torch.rand(1, 64, 56, 56)\n",
        "\n",
        "# Basic Block\n",
        "layers = nn.Sequential(\n",
        "    nn.Conv2d(dim, dim, kernel_size=3, stride=2, padding=1), # stride 2\n",
        "    nn.BatchNorm2d(dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1),\n",
        "    nn.BatchNorm2d(dim)\n",
        ")\n",
        "\n",
        "downsample = nn.Sequential(\n",
        "    nn.Conv2d(dim, dim, kernel_size=1, stride=2),\n",
        "    nn.BatchNorm2d(dim)\n",
        ")\n",
        "\n",
        "out = layers(input) + downsample(input)\n",
        "out = torch.relu(out)\n",
        "print(f\"input shape: {input.shape}\")\n",
        "print(f\"output shape: {out.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRX9e0p_ydTr"
      },
      "source": [
        "### Bottleneck Block 간단 구현\n",
        "\n",
        "Basic Block과 마찬가지로 블록을 통과한 출력과 입력은 동일한 차원이어야 한다. stride가 1인 1 x 1 합성곱 계층은 차원이 변하지 않는다. 3 x 3 합성곱 계층은 padding 1을 적용시켜 출력 차원이 변하지 않도록 구성한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXA_LHSY_kqc",
        "outputId": "5b798669-3eb2-403e-add8-b26721ce790a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input shape: torch.Size([1, 256, 56, 56])\n",
            "output shape: torch.Size([1, 256, 56, 56])\n"
          ]
        }
      ],
      "source": [
        "dim = 256\n",
        "input = torch.rand(1, 256, 56, 56)\n",
        "\n",
        "# Bottleneck Block\n",
        "layers = nn.Sequential(\n",
        "    nn.Conv2d(dim, dim//4, kernel_size=1, stride=1),\n",
        "    nn.BatchNorm2d(dim//4),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(dim//4, dim//4, kernel_size=3, stride=1, padding=1),\n",
        "    nn.BatchNorm2d(dim//4),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(dim//4, dim, kernel_size=1, stride=1),\n",
        "    nn.BatchNorm2d(dim)\n",
        ")\n",
        "\n",
        "out = layers(input) + input\n",
        "out = torch.relu(out)\n",
        "print(f\"input shape: {input.shape}\")\n",
        "print(f\"output shape: {out.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN__AFOdzsCG"
      },
      "source": [
        "#### 피처맵 차원 변화에 따른 다운 샘플링 적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5AbV0nsBLj6",
        "outputId": "5057bc9c-9526-4e1e-ff48-d05ee465e2a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input shape: torch.Size([1, 256, 56, 56])\n",
            "output shape: torch.Size([1, 256, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "dim = 256\n",
        "input = torch.rand(1, 256, 56, 56)\n",
        "\n",
        "# Bottleneck Block\n",
        "layers = nn.Sequential(\n",
        "    nn.Conv2d(dim, dim//4, kernel_size=1, stride=2), # stride 2\n",
        "    nn.BatchNorm2d(dim//4),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(dim//4, dim//4, kernel_size=3, stride=1, padding=1),\n",
        "    nn.BatchNorm2d(dim//4),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(dim//4, dim, kernel_size=1, stride=1),\n",
        "    nn.BatchNorm2d(dim)\n",
        ")\n",
        "\n",
        "downsample = nn.Sequential(\n",
        "    nn.Conv2d(dim, dim, kernel_size=1, stride=2),\n",
        "    nn.BatchNorm2d(dim)\n",
        ")\n",
        "\n",
        "out = layers(input) + downsample(input)\n",
        "out = torch.relu(out)\n",
        "print(f\"input shape: {input.shape}\")\n",
        "print(f\"output shape: {out.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuLavVuNnr_a"
      },
      "source": [
        "## Resnet Architecture\n",
        "\n",
        "- Stem(conv1), stage1(conv2_x), stage2(conv3_x), stage3(conv4_x), stage4(conv5_x), FC(Full Connected) 로 구성\n",
        "- Convolution layer과 Acivation layer 사이에 Batch Normalization(BN) 적용(3.4. Implementation 참고)\n",
        "    - Resnet 18-layer\n",
        "        - Basic Block\n",
        "        - layers [2, 2, 2, 2]\n",
        "        - 1 + 2 x (2 + 2 + 2 + 2) + 1 = 18\n",
        "    - Resnet 34-layer\n",
        "        - Basic Block\n",
        "        - layers [3, 4, 6, 3]\n",
        "        - 1 + 2 x (3 + 4 + 6 + 3) + 1 = 34\n",
        "    - Resnet 50-layer\n",
        "        - Bottleneck Block\n",
        "        - layers [3, 4, 6, 3]\n",
        "        - 1 + 3 x (3 + 4 + 6 + 3) + 1 = 50\n",
        "    - Resnet 101-layer\n",
        "        - Bottleneck Block\n",
        "        - layers [3, 4, 23, 3]\n",
        "        - 1 + 3 x (3 + 4 + 23 + 3) + 1 = 101\n",
        "    - Resnet 152-layer\n",
        "        - Bottleneck Block\n",
        "        - layers [3, 8, 36, 3]\n",
        "        - 1 + 3 x (3 + 8 + 36 + 3) + 1 = 152"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzGhhrxTCNA8"
      },
      "source": [
        "<img src=\"../images/5-Table1-1.png\" alt=\"Table1\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyVlGpfCaYDJ"
      },
      "source": [
        "## Resnet 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0apnXHHZ4DFR"
      },
      "source": [
        "### Stem Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZII222SvDNrx"
      },
      "source": [
        "- conv1:\n",
        "    - input size: 224 x 224\n",
        "    - in_channels: 3(RGB)\n",
        "    - out_channels: 64\n",
        "    - kernel_size: 7\n",
        "    - stride: 2\n",
        "    - padding: 3(calculated)\n",
        "        - output size: 112 x 112"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umtZn0o3DQ9M",
        "outputId": "d0307eb7-1467-4b21-a9d2-c192b94f262d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input shape: torch.Size([1, 3, 224, 224])\n",
            "output shape: torch.Size([1, 64, 112, 112])\n"
          ]
        }
      ],
      "source": [
        "input = torch.rand(1, 3, 224, 224)\n",
        "\n",
        "conv1 = nn.Sequential(\n",
        "    nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
        "    nn.BatchNorm2d(64)\n",
        ")\n",
        "\n",
        "out = conv1(input)\n",
        "print(f\"input shape: {input.shape}\")\n",
        "print(f\"output shape: {out.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFzyNf1iDZ32"
      },
      "source": [
        "- max_pool:\n",
        "    - input size: 112 x 112\n",
        "    - kernel_size: 3\n",
        "    - stride: 2\n",
        "    - padding: 1(calculated)\n",
        "        - output size: 56 x 56"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkJYHAW6Dlew",
        "outputId": "f58a2146-ffb5-4852-f567-d08465e6c73c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input shape: torch.Size([1, 64, 112, 112])\n",
            "output shape: torch.Size([1, 64, 56, 56])\n"
          ]
        }
      ],
      "source": [
        "input = torch.rand(1, 64, 112, 112)\n",
        "max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "out = max_pool(input)\n",
        "print(f\"input shape: {input.shape}\")\n",
        "print(f\"output shape: {out.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MDPfBqU4HsI"
      },
      "source": [
        "### Residual Block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ldbSKwJEN7Y"
      },
      "source": [
        "- Basic Block(18, 34 layer)\n",
        "    - input size: 56, 28, 14, 7\n",
        "    - kernel_size: 3\n",
        "    - stride: 1 or 2\n",
        "    - padding: 1\n",
        "        - output size: 56, 28, 14, 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XB5jTOCoi0c"
      },
      "source": [
        "- 다운 샘플링 적용(ex. 18-layer)\n",
        "    - conv2_1, conv3_1, conv4_1, conv5_1: 피처맵 크기 축소(stride=2)\n",
        "    - 위 계층에 다운 샘플링 적용\n",
        "\n",
        "\n",
        "|   conv   | in_dim | dim | out_dim | stride | downsample |\n",
        "| :------: | :----: | :-: | :----:  |:-----: | :--------: |\n",
        "| conv2_1  |   64   | 64  |   64    |  1     |     X      |\n",
        "| conv2_2  |   64   | 64  |   64    |  1     |     X      |\n",
        "| conv3_1  |   64   | 128 |   128   |  2     |     O      |\n",
        "| conv3_2  |  128   | 128 |   128   |  1     |     X      |\n",
        "| conv4_1  |  128   | 256 |   256   |  2     |     O      |\n",
        "| conv4_2  |  256   | 256 |   256   |  1     |     X      |\n",
        "| conv5_1  |  256   | 512 |   256   |  2     |     O      |\n",
        "| conv5_2  |  512   | 512 |   256   |  1     |     X      |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "E2OaBadtF6XA"
      },
      "outputs": [],
      "source": [
        "# class BasicBlock(nn.Module):\n",
        "#     def __init__(self, in_dim, dim, stride=1):\n",
        "#         super().__init__()\n",
        "#         self.conv1 = nn.Conv2d(in_dim, dim, kernel_size=3, stride=stride, padding=1)\n",
        "#         self.bn = nn.BatchNorm2d(dim)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         input = x\n",
        "#         out = self.conv1(x)\n",
        "#         out = self.bn(out)\n",
        "#         out = self.relu(out)\n",
        "#         out = self.conv2(out)\n",
        "#         out = self.bn(out)\n",
        "\n",
        "#         out = out + input\n",
        "#         out = self.relu(out)\n",
        "#         return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Yus-LvfxHm54"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_dim, dim, stride=1):\n",
        "        super().__init__()\n",
        "        self.stride = stride\n",
        "        self.conv1 = nn.Conv2d(in_dim, dim, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(dim)\n",
        "        self.downsample = self._build_downsample_layer(in_dim, dim, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.stride != 1:\n",
        "            input = self.downsample(input)\n",
        "\n",
        "        out = out + input\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "    def _build_downsample_layer(self, in_dim, dim, stride):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_dim, dim * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "            nn.BatchNorm2d(dim * self.expansion)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0iuJ4HXJOIB",
        "outputId": "13ae0cab-483d-4338-ee27-add00ac31eb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input shape: torch.Size([1, 64, 56, 56])\n",
            "output shape: torch.Size([1, 64, 56, 56])\n"
          ]
        }
      ],
      "source": [
        "input = torch.rand(1, 64, 56, 56)\n",
        "basic_block = BasicBlock(64, 64, 1)\n",
        "out = basic_block(input)\n",
        "print(f\"input shape: {input.shape}\")\n",
        "print(f\"output shape: {out.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUV3XM5cJaym",
        "outputId": "b77a8688-b962-4f3e-bcf3-0948bab9372f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input shape: torch.Size([1, 64, 56, 56])\n",
            "output shape: torch.Size([1, 128, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "input = torch.rand(1, 64, 56, 56)\n",
        "basic_block = BasicBlock(64, 128, 2)\n",
        "out = basic_block(input)\n",
        "print(f\"input shape: {input.shape}\")\n",
        "print(f\"output shape: {out.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtiQhg_riKR1"
      },
      "source": [
        "- Bottleneck Block(18, 34 layer)\n",
        "    - input size: 56, 28, 14, 7\n",
        "    - kernel_size: 1 or 3\n",
        "    - stride: 1 or 2\n",
        "    - padding: 0 or 1\n",
        "        - output size: 56, 28, 14, 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeFb410jopMB"
      },
      "source": [
        "- 다운 샘플링 적용(ex. 50-layer)\n",
        "    - conv3_1, conv4_1, conv5_1: 피처맵 크기 축소(stride=2)\n",
        "    - conv2_1: feature map channel 변화(64 -> 256)\n",
        "    - 위 계층에 다운 샘플링 적용\n",
        "\n",
        "|   conv   | in_dim | dim | out_dim | stride | downsample |\n",
        "| :------: | :----: | :-: | :-----: | :----: | :--------: |\n",
        "| conv2_1  |   64   | 64  |   256   |   1    |     O      |\n",
        "| conv2_2  |  256   | 64  |   256   |   1    |     X      |\n",
        "| conv2_3  |  256   | 64  |   256   |   1    |     X      |\n",
        "| conv3_1  |  256   | 128 |   512   |   2    |     O      |\n",
        "| conv3_2  |  512   | 128 |   512   |   1    |     X      |\n",
        "| conv3_3  |  512   | 128 |   512   |   1    |     X      |\n",
        "| conv3_4  |  512   | 128 |   512   |   1    |     X      |\n",
        "| conv4_1  |  512   | 256 |   1024  |   2    |     O      |\n",
        "| conv4_2  |  1024  | 256 |   1024  |   1    |     X      |\n",
        "| conv4_3  |  1024  | 256 |   1024  |   1    |     X      |\n",
        "| conv4_4  |  1024  | 256 |   1024  |   1    |     X      |\n",
        "| conv4_5  |  1024  | 256 |   1024  |   1    |     X      |\n",
        "| conv4_6  |  1024  | 256 |   1024  |   1    |     X      |\n",
        "| conv5_1  |  1024  | 512 |   2048  |   2    |     O      |\n",
        "| conv5_2  |  2048  | 512 |   2048  |   1    |     X      |\n",
        "| conv5_3  |  2048  | 512 |   2048  |   1    |     X      |\n",
        "| conv5_4  |  2048  | 512 |   2048  |   1    |     X      |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "M6bGAr3RQMaY"
      },
      "outputs": [],
      "source": [
        "# class BottleneckBlock(nn.Module):\n",
        "#     def __init__(self, in_dim, dim, stride=1):\n",
        "#         super().__init__()\n",
        "#         self.stride = stride\n",
        "#         self.conv1 = nn.Conv2d(in_dim, dim, kernel_size=1, stride=stride, padding=0)\n",
        "#         self.bn1 = nn.BatchNorm2d(dim)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1)\n",
        "#         self.bn2 = nn.BatchNorm2d(dim)\n",
        "#         self.conv3 = nn.Conv2d(dim, dim * 4, kernel_size=1, stride=1, padding=0)\n",
        "#         self.bn3 = nn.BatchNorm2d(dim * 4)\n",
        "#         self.downsample = self._build_downsample_layer(in_dim, dim, stride)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         input = x\n",
        "#         out = self.conv1(x)\n",
        "#         out = self.bn1(out)\n",
        "#         out = self.relu(out)\n",
        "#         out = self.conv2(out)\n",
        "#         out = self.bn2(out)\n",
        "#         out = self.relu(out)\n",
        "#         out = self.conv3(out)\n",
        "#         out = self.bn3(out)\n",
        "\n",
        "#         if self.stride != 1:\n",
        "#             input = self.downsample(input)\n",
        "\n",
        "#         out = out + input\n",
        "#         out = self.relu(out)\n",
        "#         return out\n",
        "\n",
        "#     def _build_downsample_layer(self, in_dim, dim, stride):\n",
        "#         return nn.Sequential(\n",
        "#             nn.Conv2d(in_dim, dim * 4, kernel_size=1, stride=stride),\n",
        "#             nn.BatchNorm2d(dim * 4)\n",
        "#         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Lh2rTIs-LXv2"
      },
      "outputs": [],
      "source": [
        "class BottleneckBlock(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_dim, dim, stride=1):\n",
        "        super().__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.dim = dim\n",
        "        self.stride = stride\n",
        "        self.conv1 = nn.Conv2d(in_dim, dim, kernel_size=1, stride=stride, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(dim)\n",
        "        self.conv3 = nn.Conv2d(dim, dim * self.expansion, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(dim * self.expansion)\n",
        "        self.downsample = self._build_downsample_layer(in_dim, dim, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.stride != 1 or self.in_dim != self.dim * self.expansion:\n",
        "            input = self.downsample(input)\n",
        "\n",
        "        out = out + input\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "    def _build_downsample_layer(self, in_dim, dim, stride):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_dim, dim * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "            nn.BatchNorm2d(dim * self.expansion)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHN9R_GrMVVw",
        "outputId": "9c4b1184-94ba-4d03-df70-7860dd93bd3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input shape: torch.Size([1, 64, 56, 56])\n",
            "output shape: torch.Size([1, 256, 56, 56])\n"
          ]
        }
      ],
      "source": [
        "input = torch.rand(1, 64, 56, 56)\n",
        "\n",
        "bottleneck_block = BottleneckBlock(64, 64, 1)\n",
        "out = bottleneck_block(input)\n",
        "print(f\"input shape: {input.shape}\")\n",
        "print(f\"output shape: {out.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "462di52_PmhN",
        "outputId": "457744f0-566f-48bc-907d-62464813978f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input shape: torch.Size([1, 256, 56, 56])\n",
            "output shape: torch.Size([1, 512, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "input = torch.rand(1, 256, 56, 56)\n",
        "\n",
        "bottleneck_block = BottleneckBlock(256, 128, 2)\n",
        "out = bottleneck_block(input)\n",
        "print(f\"input shape: {input.shape}\")\n",
        "print(f\"output shape: {out.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQARCRF54Mwy"
      },
      "source": [
        "### ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "N0AEAUckiJIr"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 224\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.in_dim = 64\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.stage1 = self._build_stage(block, 64, layers[0], stride=1)\n",
        "        self.stage2 = self._build_stage(block, 128, layers[1], stride=2)\n",
        "        self.stage3 = self._build_stage(block, 256, layers[2], stride=2)\n",
        "        self.stage4 = self._build_stage(block, 512, layers[3], stride=2)\n",
        "\n",
        "        self.average_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        # for layers in [self.stem, self.stage1, self.stage2, self.stage3, self.stage4, self.fc]:\n",
        "        #     if isinstance(layers, nn.Linear):\n",
        "        #         nn.init.xavier_uniform_(layers.weight)\n",
        "        #         continue\n",
        "\n",
        "        #     for layer in layers:\n",
        "        #         if isinstance(layer, nn.Conv2d):\n",
        "        #             nn.init.(layer.weight)\n",
        "\n",
        "    def _build_stage(self, block, dim, layers, stride):\n",
        "        stage = []\n",
        "        stage.append(block(self.in_dim, dim, stride))\n",
        "        self.in_dim = dim * block.expansion\n",
        "\n",
        "        for _ in range(layers - 1):\n",
        "            stage.append(block(self.in_dim, dim, stride=1))\n",
        "\n",
        "        return nn.Sequential(*stage)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.stem(x)\n",
        "        out = self.max_pool(out)\n",
        "        out = self.stage1(out)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = self.stage4(out)\n",
        "        out = self.average_pool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJgZ9rS0o7QB",
        "outputId": "6e47e054-a96b-4963-d299-7fe9657ba1be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 1000])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resnet18 = ResNet(BasicBlock, [2, 2, 2, 2], 1000)\n",
        "x = torch.rand(1, 3, 224, 224)\n",
        "out = resnet18(x)\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSbqLYxj4Rvv"
      },
      "source": [
        "### 모델 요약"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NJA5jtEkzj1r"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from torchinfo import summary\n",
        "except Exception as ex:\n",
        "    !pip install torchinfo\n",
        "    from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twb0BVj7ziN2",
        "outputId": "f1c3b7d0-ef03-4b1a-fbe0-588438cd8334"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "ResNet                                   [1, 1000]                 --\n",
              "├─Sequential: 1-1                        [1, 64, 112, 112]         --\n",
              "│    └─Conv2d: 2-1                       [1, 64, 112, 112]         9,408\n",
              "│    └─BatchNorm2d: 2-2                  [1, 64, 112, 112]         128\n",
              "├─MaxPool2d: 1-2                         [1, 64, 56, 56]           --\n",
              "├─Sequential: 1-3                        [1, 64, 56, 56]           --\n",
              "│    └─BasicBlock: 2-3                   [1, 64, 56, 56]           4,224\n",
              "│    │    └─Conv2d: 3-1                  [1, 64, 56, 56]           36,864\n",
              "│    │    └─BatchNorm2d: 3-2             [1, 64, 56, 56]           128\n",
              "│    │    └─ReLU: 3-3                    [1, 64, 56, 56]           --\n",
              "│    │    └─Conv2d: 3-4                  [1, 64, 56, 56]           36,864\n",
              "│    │    └─BatchNorm2d: 3-5             [1, 64, 56, 56]           128\n",
              "│    │    └─ReLU: 3-6                    [1, 64, 56, 56]           --\n",
              "│    └─BasicBlock: 2-4                   [1, 64, 56, 56]           4,224\n",
              "│    │    └─Conv2d: 3-7                  [1, 64, 56, 56]           36,864\n",
              "│    │    └─BatchNorm2d: 3-8             [1, 64, 56, 56]           128\n",
              "│    │    └─ReLU: 3-9                    [1, 64, 56, 56]           --\n",
              "│    │    └─Conv2d: 3-10                 [1, 64, 56, 56]           36,864\n",
              "│    │    └─BatchNorm2d: 3-11            [1, 64, 56, 56]           128\n",
              "│    │    └─ReLU: 3-12                   [1, 64, 56, 56]           --\n",
              "├─Sequential: 1-4                        [1, 128, 28, 28]          --\n",
              "│    └─BasicBlock: 2-5                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-13                 [1, 128, 28, 28]          73,728\n",
              "│    │    └─BatchNorm2d: 3-14            [1, 128, 28, 28]          256\n",
              "│    │    └─ReLU: 3-15                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-16                 [1, 128, 28, 28]          147,456\n",
              "│    │    └─BatchNorm2d: 3-17            [1, 128, 28, 28]          256\n",
              "│    │    └─Sequential: 3-18             [1, 128, 28, 28]          8,448\n",
              "│    │    └─ReLU: 3-19                   [1, 128, 28, 28]          --\n",
              "│    └─BasicBlock: 2-6                   [1, 128, 28, 28]          16,640\n",
              "│    │    └─Conv2d: 3-20                 [1, 128, 28, 28]          147,456\n",
              "│    │    └─BatchNorm2d: 3-21            [1, 128, 28, 28]          256\n",
              "│    │    └─ReLU: 3-22                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-23                 [1, 128, 28, 28]          147,456\n",
              "│    │    └─BatchNorm2d: 3-24            [1, 128, 28, 28]          256\n",
              "│    │    └─ReLU: 3-25                   [1, 128, 28, 28]          --\n",
              "├─Sequential: 1-5                        [1, 256, 14, 14]          --\n",
              "│    └─BasicBlock: 2-7                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-26                 [1, 256, 14, 14]          294,912\n",
              "│    │    └─BatchNorm2d: 3-27            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-28                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-29                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-30            [1, 256, 14, 14]          512\n",
              "│    │    └─Sequential: 3-31             [1, 256, 14, 14]          33,280\n",
              "│    │    └─ReLU: 3-32                   [1, 256, 14, 14]          --\n",
              "│    └─BasicBlock: 2-8                   [1, 256, 14, 14]          66,048\n",
              "│    │    └─Conv2d: 3-33                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-34            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-35                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-36                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-37            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-38                   [1, 256, 14, 14]          --\n",
              "├─Sequential: 1-6                        [1, 512, 7, 7]            --\n",
              "│    └─BasicBlock: 2-9                   [1, 512, 7, 7]            --\n",
              "│    │    └─Conv2d: 3-39                 [1, 512, 7, 7]            1,179,648\n",
              "│    │    └─BatchNorm2d: 3-40            [1, 512, 7, 7]            1,024\n",
              "│    │    └─ReLU: 3-41                   [1, 512, 7, 7]            --\n",
              "│    │    └─Conv2d: 3-42                 [1, 512, 7, 7]            2,359,296\n",
              "│    │    └─BatchNorm2d: 3-43            [1, 512, 7, 7]            1,024\n",
              "│    │    └─Sequential: 3-44             [1, 512, 7, 7]            132,096\n",
              "│    │    └─ReLU: 3-45                   [1, 512, 7, 7]            --\n",
              "│    └─BasicBlock: 2-10                  [1, 512, 7, 7]            263,168\n",
              "│    │    └─Conv2d: 3-46                 [1, 512, 7, 7]            2,359,296\n",
              "│    │    └─BatchNorm2d: 3-47            [1, 512, 7, 7]            1,024\n",
              "│    │    └─ReLU: 3-48                   [1, 512, 7, 7]            --\n",
              "│    │    └─Conv2d: 3-49                 [1, 512, 7, 7]            2,359,296\n",
              "│    │    └─BatchNorm2d: 3-50            [1, 512, 7, 7]            1,024\n",
              "│    │    └─ReLU: 3-51                   [1, 512, 7, 7]            --\n",
              "├─AdaptiveAvgPool2d: 1-7                 [1, 512, 1, 1]            --\n",
              "├─Linear: 1-8                            [1, 1000]                 513,000\n",
              "==========================================================================================\n",
              "Total params: 12,043,816\n",
              "Trainable params: 12,043,816\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 1.81\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 39.75\n",
              "Params size (MB): 46.76\n",
              "Estimated Total Size (MB): 87.11\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(resnet18, [1, 3, 224, 224])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zvLWd_pzpqKc"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "53ScFmjpsOBF"
      },
      "outputs": [],
      "source": [
        "num_classes = 1000\n",
        "\n",
        "resnet50 = ResNet(BottleneckBlock, [3, 4, 6, 3], num_classes)\n",
        "x = torch.rand(1, 3, 224, 224)\n",
        "out = resnet50(x)\n",
        "assert out.shape == torch.Size([1, num_classes])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sEIPVS7sffz",
        "outputId": "a7e8732e-4742-41a8-b267-62041cff3269"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "conv2d() received an invalid combination of arguments - got (int, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!int!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!int!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m summary(\u001b[43mresnet50\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m, [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m])\n",
            "File \u001b[1;32mc:\\Users\\nagix\\Documents\\gnkim\\resnet\\backend\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\nagix\\Documents\\gnkim\\resnet\\backend\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[19], line 41\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 41\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_pool(out)\n\u001b[0;32m     43\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage1(out)\n",
            "File \u001b[1;32mc:\\Users\\nagix\\Documents\\gnkim\\resnet\\backend\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\nagix\\Documents\\gnkim\\resnet\\backend\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\nagix\\Documents\\gnkim\\resnet\\backend\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\nagix\\Documents\\gnkim\\resnet\\backend\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\nagix\\Documents\\gnkim\\resnet\\backend\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\nagix\\Documents\\gnkim\\resnet\\backend\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\nagix\\Documents\\gnkim\\resnet\\backend\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (int, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!int!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!int!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n"
          ]
        }
      ],
      "source": [
        "summary(resnet50(1000), [1, 3, 224, 224])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR_7FvdFro7Z",
        "outputId": "faac257e-3011-4c2a-8599-22858ebbda87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "ResNet                                   [1, 1000]                 --\n",
              "├─Conv2d: 1-1                            [1, 64, 112, 112]         9,408\n",
              "├─BatchNorm2d: 1-2                       [1, 64, 112, 112]         128\n",
              "├─ReLU: 1-3                              [1, 64, 112, 112]         --\n",
              "├─MaxPool2d: 1-4                         [1, 64, 56, 56]           --\n",
              "├─Sequential: 1-5                        [1, 64, 56, 56]           --\n",
              "│    └─BasicBlock: 2-1                   [1, 64, 56, 56]           --\n",
              "│    │    └─Conv2d: 3-1                  [1, 64, 56, 56]           36,864\n",
              "│    │    └─BatchNorm2d: 3-2             [1, 64, 56, 56]           128\n",
              "│    │    └─ReLU: 3-3                    [1, 64, 56, 56]           --\n",
              "│    │    └─Conv2d: 3-4                  [1, 64, 56, 56]           36,864\n",
              "│    │    └─BatchNorm2d: 3-5             [1, 64, 56, 56]           128\n",
              "│    │    └─ReLU: 3-6                    [1, 64, 56, 56]           --\n",
              "│    └─BasicBlock: 2-2                   [1, 64, 56, 56]           --\n",
              "│    │    └─Conv2d: 3-7                  [1, 64, 56, 56]           36,864\n",
              "│    │    └─BatchNorm2d: 3-8             [1, 64, 56, 56]           128\n",
              "│    │    └─ReLU: 3-9                    [1, 64, 56, 56]           --\n",
              "│    │    └─Conv2d: 3-10                 [1, 64, 56, 56]           36,864\n",
              "│    │    └─BatchNorm2d: 3-11            [1, 64, 56, 56]           128\n",
              "│    │    └─ReLU: 3-12                   [1, 64, 56, 56]           --\n",
              "│    └─BasicBlock: 2-3                   [1, 64, 56, 56]           --\n",
              "│    │    └─Conv2d: 3-13                 [1, 64, 56, 56]           36,864\n",
              "│    │    └─BatchNorm2d: 3-14            [1, 64, 56, 56]           128\n",
              "│    │    └─ReLU: 3-15                   [1, 64, 56, 56]           --\n",
              "│    │    └─Conv2d: 3-16                 [1, 64, 56, 56]           36,864\n",
              "│    │    └─BatchNorm2d: 3-17            [1, 64, 56, 56]           128\n",
              "│    │    └─ReLU: 3-18                   [1, 64, 56, 56]           --\n",
              "├─Sequential: 1-6                        [1, 128, 28, 28]          --\n",
              "│    └─BasicBlock: 2-4                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-19                 [1, 128, 28, 28]          73,728\n",
              "│    │    └─BatchNorm2d: 3-20            [1, 128, 28, 28]          256\n",
              "│    │    └─ReLU: 3-21                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-22                 [1, 128, 28, 28]          147,456\n",
              "│    │    └─BatchNorm2d: 3-23            [1, 128, 28, 28]          256\n",
              "│    │    └─Sequential: 3-24             [1, 128, 28, 28]          8,448\n",
              "│    │    └─ReLU: 3-25                   [1, 128, 28, 28]          --\n",
              "│    └─BasicBlock: 2-5                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-26                 [1, 128, 28, 28]          147,456\n",
              "│    │    └─BatchNorm2d: 3-27            [1, 128, 28, 28]          256\n",
              "│    │    └─ReLU: 3-28                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-29                 [1, 128, 28, 28]          147,456\n",
              "│    │    └─BatchNorm2d: 3-30            [1, 128, 28, 28]          256\n",
              "│    │    └─ReLU: 3-31                   [1, 128, 28, 28]          --\n",
              "│    └─BasicBlock: 2-6                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-32                 [1, 128, 28, 28]          147,456\n",
              "│    │    └─BatchNorm2d: 3-33            [1, 128, 28, 28]          256\n",
              "│    │    └─ReLU: 3-34                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-35                 [1, 128, 28, 28]          147,456\n",
              "│    │    └─BatchNorm2d: 3-36            [1, 128, 28, 28]          256\n",
              "│    │    └─ReLU: 3-37                   [1, 128, 28, 28]          --\n",
              "│    └─BasicBlock: 2-7                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-38                 [1, 128, 28, 28]          147,456\n",
              "│    │    └─BatchNorm2d: 3-39            [1, 128, 28, 28]          256\n",
              "│    │    └─ReLU: 3-40                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-41                 [1, 128, 28, 28]          147,456\n",
              "│    │    └─BatchNorm2d: 3-42            [1, 128, 28, 28]          256\n",
              "│    │    └─ReLU: 3-43                   [1, 128, 28, 28]          --\n",
              "├─Sequential: 1-7                        [1, 256, 14, 14]          --\n",
              "│    └─BasicBlock: 2-8                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-44                 [1, 256, 14, 14]          294,912\n",
              "│    │    └─BatchNorm2d: 3-45            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-46                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-47                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-48            [1, 256, 14, 14]          512\n",
              "│    │    └─Sequential: 3-49             [1, 256, 14, 14]          33,280\n",
              "│    │    └─ReLU: 3-50                   [1, 256, 14, 14]          --\n",
              "│    └─BasicBlock: 2-9                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-51                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-52            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-53                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-54                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-55            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-56                   [1, 256, 14, 14]          --\n",
              "│    └─BasicBlock: 2-10                  [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-57                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-58            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-59                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-60                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-61            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-62                   [1, 256, 14, 14]          --\n",
              "│    └─BasicBlock: 2-11                  [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-63                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-64            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-65                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-66                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-67            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-68                   [1, 256, 14, 14]          --\n",
              "│    └─BasicBlock: 2-12                  [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-69                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-70            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-71                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-72                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-73            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-74                   [1, 256, 14, 14]          --\n",
              "│    └─BasicBlock: 2-13                  [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-75                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-76            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-77                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-78                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-79            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-80                   [1, 256, 14, 14]          --\n",
              "├─Sequential: 1-8                        [1, 512, 7, 7]            --\n",
              "│    └─BasicBlock: 2-14                  [1, 512, 7, 7]            --\n",
              "│    │    └─Conv2d: 3-81                 [1, 512, 7, 7]            1,179,648\n",
              "│    │    └─BatchNorm2d: 3-82            [1, 512, 7, 7]            1,024\n",
              "│    │    └─ReLU: 3-83                   [1, 512, 7, 7]            --\n",
              "│    │    └─Conv2d: 3-84                 [1, 512, 7, 7]            2,359,296\n",
              "│    │    └─BatchNorm2d: 3-85            [1, 512, 7, 7]            1,024\n",
              "│    │    └─Sequential: 3-86             [1, 512, 7, 7]            132,096\n",
              "│    │    └─ReLU: 3-87                   [1, 512, 7, 7]            --\n",
              "│    └─BasicBlock: 2-15                  [1, 512, 7, 7]            --\n",
              "│    │    └─Conv2d: 3-88                 [1, 512, 7, 7]            2,359,296\n",
              "│    │    └─BatchNorm2d: 3-89            [1, 512, 7, 7]            1,024\n",
              "│    │    └─ReLU: 3-90                   [1, 512, 7, 7]            --\n",
              "│    │    └─Conv2d: 3-91                 [1, 512, 7, 7]            2,359,296\n",
              "│    │    └─BatchNorm2d: 3-92            [1, 512, 7, 7]            1,024\n",
              "│    │    └─ReLU: 3-93                   [1, 512, 7, 7]            --\n",
              "│    └─BasicBlock: 2-16                  [1, 512, 7, 7]            --\n",
              "│    │    └─Conv2d: 3-94                 [1, 512, 7, 7]            2,359,296\n",
              "│    │    └─BatchNorm2d: 3-95            [1, 512, 7, 7]            1,024\n",
              "│    │    └─ReLU: 3-96                   [1, 512, 7, 7]            --\n",
              "│    │    └─Conv2d: 3-97                 [1, 512, 7, 7]            2,359,296\n",
              "│    │    └─BatchNorm2d: 3-98            [1, 512, 7, 7]            1,024\n",
              "│    │    └─ReLU: 3-99                   [1, 512, 7, 7]            --\n",
              "├─AdaptiveAvgPool2d: 1-9                 [1, 512, 1, 1]            --\n",
              "├─Linear: 1-10                           [1, 1000]                 513,000\n",
              "==========================================================================================\n",
              "Total params: 21,797,672\n",
              "Trainable params: 21,797,672\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 3.66\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 59.82\n",
              "Params size (MB): 87.19\n",
              "Estimated Total Size (MB): 147.61\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torchvision\n",
        "\n",
        "summary(torchvision.models.resnet34(), [1, 3, 224, 224])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Md7uakC64dsq"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_dim, dim, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_dim, dim, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(dim)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            input = self.downsample(input)\n",
        "\n",
        "        out = out + input\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# def _build_downsample_layer(self, in_dim, dim, stride):\n",
        "#     return nn.Sequential(\n",
        "#         nn.Conv2d(in_dim, dim * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "#         nn.BatchNorm2d(dim * self.expansion)\n",
        "#     )\n",
        "\n",
        "class BottleneckBlock(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_dim, dim, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_dim, dim, kernel_size=1, stride=stride, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(dim)\n",
        "        self.conv3 = nn.Conv2d(dim, dim * self.expansion, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(dim * self.expansion)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            input = self.downsample(input)\n",
        "\n",
        "        out = out + input\n",
        "        out = self.relu(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "RKoA0l0R4dqA"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.in_dim = 64\n",
        "        self.downsample = None\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.stage1 = self._build_stage(block, 64, layers[0], stride=1)\n",
        "        self.stage2 = self._build_stage(block, 128, layers[1], stride=2)\n",
        "        self.stage3 = self._build_stage(block, 256, layers[2], stride=2)\n",
        "        self.stage4 = self._build_stage(block, 512, layers[3], stride=2)\n",
        "\n",
        "        self.average_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _build_stage(self, block, dim, layers, stride):\n",
        "        stage = []\n",
        "\n",
        "        if stride != 1 or self.in_dim != dim * block.expansion:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_dim, dim * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(dim * block.expansion)\n",
        "            )\n",
        "\n",
        "        stage.append(block(self.in_dim, dim, stride, self.downsample))\n",
        "        self.in_dim = dim * block.expansion\n",
        "\n",
        "        for _ in range(layers - 1):\n",
        "            stage.append(block(self.in_dim, dim, stride=1))\n",
        "\n",
        "        return nn.Sequential(*stage)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.stem(x)\n",
        "        out = self.max_pool(out)\n",
        "        out = self.stage1(out)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = self.stage4(out)\n",
        "        out = self.average_pool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "f7w6-id-4dni"
      },
      "outputs": [],
      "source": [
        "def resnet18(num_classes):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
        "\n",
        "def resnet34(num_classes):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n",
        "\n",
        "def resnet50(num_classes):\n",
        "    return ResNet(BottleneckBlock, [3, 4, 6, 3], num_classes)\n",
        "\n",
        "def resnet101(num_classes):\n",
        "    return ResNet(BottleneckBlock, [3, 4, 23, 3], num_classes)\n",
        "\n",
        "def resnet152(num_classes):\n",
        "    return ResNet(BottleneckBlock, [3, 8, 36, 3], num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHlImAYa5yuS",
        "outputId": "6384fc2d-9b79-4795-d03e-1b72399b0d88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "ResNet                                   [1, 1000]                 --\n",
              "├─Sequential: 1-1                        [1, 64, 112, 112]         --\n",
              "│    └─Conv2d: 2-1                       [1, 64, 112, 112]         9,408\n",
              "│    └─BatchNorm2d: 2-2                  [1, 64, 112, 112]         128\n",
              "├─MaxPool2d: 1-2                         [1, 64, 56, 56]           --\n",
              "├─Sequential: 1-3                        [1, 64, 56, 56]           --\n",
              "│    └─BasicBlock: 2-3                   [1, 64, 56, 56]           --\n",
              "│    │    └─Conv2d: 3-1                  [1, 64, 56, 56]           36,864\n",
              "│    │    └─BatchNorm2d: 3-2             [1, 64, 56, 56]           128\n",
              "│    │    └─ReLU: 3-3                    [1, 64, 56, 56]           --\n",
              "│    │    └─Conv2d: 3-4                  [1, 64, 56, 56]           36,864\n",
              "│    │    └─BatchNorm2d: 3-5             [1, 64, 56, 56]           128\n",
              "│    │    └─ReLU: 3-6                    [1, 64, 56, 56]           --\n",
              "│    └─BasicBlock: 2-4                   [1, 64, 56, 56]           --\n",
              "│    │    └─Conv2d: 3-7                  [1, 64, 56, 56]           36,864\n",
              "│    │    └─BatchNorm2d: 3-8             [1, 64, 56, 56]           128\n",
              "│    │    └─ReLU: 3-9                    [1, 64, 56, 56]           --\n",
              "│    │    └─Conv2d: 3-10                 [1, 64, 56, 56]           36,864\n",
              "│    │    └─BatchNorm2d: 3-11            [1, 64, 56, 56]           128\n",
              "│    │    └─ReLU: 3-12                   [1, 64, 56, 56]           --\n",
              "│    └─BasicBlock: 2-5                   [1, 64, 56, 56]           --\n",
              "│    │    └─Conv2d: 3-13                 [1, 64, 56, 56]           36,864\n",
              "│    │    └─BatchNorm2d: 3-14            [1, 64, 56, 56]           128\n",
              "│    │    └─ReLU: 3-15                   [1, 64, 56, 56]           --\n",
              "│    │    └─Conv2d: 3-16                 [1, 64, 56, 56]           36,864\n",
              "│    │    └─BatchNorm2d: 3-17            [1, 64, 56, 56]           128\n",
              "│    │    └─ReLU: 3-18                   [1, 64, 56, 56]           --\n",
              "├─Sequential: 1-4                        [1, 128, 28, 28]          --\n",
              "│    └─BasicBlock: 2-6                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-19                 [1, 128, 28, 28]          73,728\n",
              "│    │    └─BatchNorm2d: 3-20            [1, 128, 28, 28]          256\n",
              "│    │    └─ReLU: 3-21                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-22                 [1, 128, 28, 28]          147,456\n",
              "│    │    └─BatchNorm2d: 3-23            [1, 128, 28, 28]          256\n",
              "│    │    └─Sequential: 3-24             [1, 128, 28, 28]          8,448\n",
              "│    │    └─ReLU: 3-25                   [1, 128, 28, 28]          --\n",
              "│    └─BasicBlock: 2-7                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-26                 [1, 128, 28, 28]          147,456\n",
              "│    │    └─BatchNorm2d: 3-27            [1, 128, 28, 28]          256\n",
              "│    │    └─ReLU: 3-28                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-29                 [1, 128, 28, 28]          147,456\n",
              "│    │    └─BatchNorm2d: 3-30            [1, 128, 28, 28]          256\n",
              "│    │    └─ReLU: 3-31                   [1, 128, 28, 28]          --\n",
              "│    └─BasicBlock: 2-8                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-32                 [1, 128, 28, 28]          147,456\n",
              "│    │    └─BatchNorm2d: 3-33            [1, 128, 28, 28]          256\n",
              "│    │    └─ReLU: 3-34                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-35                 [1, 128, 28, 28]          147,456\n",
              "│    │    └─BatchNorm2d: 3-36            [1, 128, 28, 28]          256\n",
              "│    │    └─ReLU: 3-37                   [1, 128, 28, 28]          --\n",
              "│    └─BasicBlock: 2-9                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-38                 [1, 128, 28, 28]          147,456\n",
              "│    │    └─BatchNorm2d: 3-39            [1, 128, 28, 28]          256\n",
              "│    │    └─ReLU: 3-40                   [1, 128, 28, 28]          --\n",
              "│    │    └─Conv2d: 3-41                 [1, 128, 28, 28]          147,456\n",
              "│    │    └─BatchNorm2d: 3-42            [1, 128, 28, 28]          256\n",
              "│    │    └─ReLU: 3-43                   [1, 128, 28, 28]          --\n",
              "├─Sequential: 1-5                        [1, 256, 14, 14]          --\n",
              "│    └─BasicBlock: 2-10                  [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-44                 [1, 256, 14, 14]          294,912\n",
              "│    │    └─BatchNorm2d: 3-45            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-46                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-47                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-48            [1, 256, 14, 14]          512\n",
              "│    │    └─Sequential: 3-49             [1, 256, 14, 14]          33,280\n",
              "│    │    └─ReLU: 3-50                   [1, 256, 14, 14]          --\n",
              "│    └─BasicBlock: 2-11                  [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-51                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-52            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-53                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-54                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-55            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-56                   [1, 256, 14, 14]          --\n",
              "│    └─BasicBlock: 2-12                  [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-57                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-58            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-59                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-60                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-61            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-62                   [1, 256, 14, 14]          --\n",
              "│    └─BasicBlock: 2-13                  [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-63                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-64            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-65                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-66                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-67            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-68                   [1, 256, 14, 14]          --\n",
              "│    └─BasicBlock: 2-14                  [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-69                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-70            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-71                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-72                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-73            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-74                   [1, 256, 14, 14]          --\n",
              "│    └─BasicBlock: 2-15                  [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-75                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-76            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-77                   [1, 256, 14, 14]          --\n",
              "│    │    └─Conv2d: 3-78                 [1, 256, 14, 14]          589,824\n",
              "│    │    └─BatchNorm2d: 3-79            [1, 256, 14, 14]          512\n",
              "│    │    └─ReLU: 3-80                   [1, 256, 14, 14]          --\n",
              "├─Sequential: 1-6                        [1, 512, 7, 7]            --\n",
              "│    └─BasicBlock: 2-16                  [1, 512, 7, 7]            --\n",
              "│    │    └─Conv2d: 3-81                 [1, 512, 7, 7]            1,179,648\n",
              "│    │    └─BatchNorm2d: 3-82            [1, 512, 7, 7]            1,024\n",
              "│    │    └─ReLU: 3-83                   [1, 512, 7, 7]            --\n",
              "│    │    └─Conv2d: 3-84                 [1, 512, 7, 7]            2,359,296\n",
              "│    │    └─BatchNorm2d: 3-85            [1, 512, 7, 7]            1,024\n",
              "│    │    └─Sequential: 3-86             [1, 512, 7, 7]            132,096\n",
              "│    │    └─ReLU: 3-87                   [1, 512, 7, 7]            --\n",
              "│    └─BasicBlock: 2-17                  [1, 512, 7, 7]            --\n",
              "│    │    └─Conv2d: 3-88                 [1, 512, 7, 7]            2,359,296\n",
              "│    │    └─BatchNorm2d: 3-89            [1, 512, 7, 7]            1,024\n",
              "│    │    └─ReLU: 3-90                   [1, 512, 7, 7]            --\n",
              "│    │    └─Conv2d: 3-91                 [1, 512, 7, 7]            2,359,296\n",
              "│    │    └─BatchNorm2d: 3-92            [1, 512, 7, 7]            1,024\n",
              "│    │    └─ReLU: 3-93                   [1, 512, 7, 7]            --\n",
              "│    └─BasicBlock: 2-18                  [1, 512, 7, 7]            --\n",
              "│    │    └─Conv2d: 3-94                 [1, 512, 7, 7]            2,359,296\n",
              "│    │    └─BatchNorm2d: 3-95            [1, 512, 7, 7]            1,024\n",
              "│    │    └─ReLU: 3-96                   [1, 512, 7, 7]            --\n",
              "│    │    └─Conv2d: 3-97                 [1, 512, 7, 7]            2,359,296\n",
              "│    │    └─BatchNorm2d: 3-98            [1, 512, 7, 7]            1,024\n",
              "│    │    └─ReLU: 3-99                   [1, 512, 7, 7]            --\n",
              "├─AdaptiveAvgPool2d: 1-7                 [1, 512, 1, 1]            --\n",
              "├─Linear: 1-8                            [1, 1000]                 513,000\n",
              "==========================================================================================\n",
              "Total params: 21,797,672\n",
              "Trainable params: 21,797,672\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 3.66\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 59.82\n",
              "Params size (MB): 87.19\n",
              "Estimated Total Size (MB): 147.61\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(resnet34(1000), [1, 3, 224, 224])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1_FZvCxocY9"
      },
      "source": [
        "## Cifar-10 데이터 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ql6zhliJxTHw",
        "outputId": "04cc3d3c-f92c-4556-b0ae-3bf103f5fd28"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
            "��ġ ������ �ƴմϴ�.\n",
            "tar: Error opening archive: Failed to open './cifar-10-python.tar.gz'\n"
          ]
        }
      ],
      "source": [
        "# download cifar-10 dataset\n",
        "!wget -O ./cifar-10-python.tar.gz https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "!tar -xf ./cifar-10-python.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ziCItpBWOqi3"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "nk2xwnSnPJXS"
      },
      "outputs": [],
      "source": [
        "class CifarDataset(Dataset):\n",
        "    def __init__(self, cifar_path, transform=None, train=True):\n",
        "        filename = \"data_batch_*\" if train else \"test_batch\"\n",
        "        batch_files = list(Path(cifar_path).glob(filename))\n",
        "\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.transform = transform\n",
        "\n",
        "        for file in batch_files:\n",
        "            batch = self.unpickle(str(file))\n",
        "            self.labels += batch[b'labels']\n",
        "            data = batch[b'data']\n",
        "\n",
        "            for image in data:\n",
        "                image = [np.split(x, 32) for x in np.split(image, 3)]\n",
        "                image = np.array(image)\n",
        "                self.images.append(image)\n",
        "\n",
        "        self.images = torch.from_numpy(np.array(self.images))\n",
        "\n",
        "    def unpickle(self, file):\n",
        "        import pickle\n",
        "        with open(file, 'rb') as fo:\n",
        "            dict = pickle.load(fo, encoding='bytes')\n",
        "        return dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "euWq78WQWXqk"
      },
      "outputs": [],
      "source": [
        "transform = v2.Compose([\n",
        "    v2.RandomChoice([v2.Resize(256), v2.Resize(480)]),\n",
        "    v2.Resize((224, 224)),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "0nzOGbx0SMhB"
      },
      "outputs": [],
      "source": [
        "train_dataset = CifarDataset(\"../data/train\", transform=transform, train=True)\n",
        "test_dataset = CifarDataset(\"../data/test\", transform=transform, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "DFGtL5Z4SSTx"
      },
      "outputs": [],
      "source": [
        "assert len(train_dataset) == 50_000\n",
        "assert len(test_dataset) == 10_000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "PhJG6uSyb5UI"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_dataset, validation_dataset = random_split(train_dataset, [0.8, 0.2])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ORMy20HvYGIX"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "model_path = Path(\"../models\")\n",
        "model_path.mkdir(exist_ok=True)\n",
        "model_name = \"RESNET50.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "XFMt-8quXzqB"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[52], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     31\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 32\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
            "File \u001b[1;32mc:\\Users\\nagix\\Documents\\gnkim\\resnet\\backend\\venv\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\nagix\\Documents\\gnkim\\resnet\\backend\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\nagix\\Documents\\gnkim\\resnet\\backend\\venv\\lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "epochs = 20\n",
        "num_classes = 10\n",
        "lr = 1e-5\n",
        "\n",
        "best_loss = sys.maxsize\n",
        "\n",
        "model = resnet50(num_classes).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "\n",
        "train_loss_hist, test_loss_hist = [], []\n",
        "train_accuracy_hist, test_accuracy_hist = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss, test_loss = 0, 0\n",
        "    train_accuracy, test_accuracy = 0, 0\n",
        "\n",
        "    model.train()\n",
        "    for X_train, y_train in train_dataloader:\n",
        "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "\n",
        "        pred_logits = model(X_train)\n",
        "        preds = torch.argmax(pred_logits, dim=1)\n",
        "        train_accuracy += (preds == y_train).sum().item()\n",
        "        loss = loss_fn(pred_logits, y_train)\n",
        "        train_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X_test, y_test in validation_dataloader:\n",
        "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "            pred_logits = model(X_test)\n",
        "            preds = torch.argmax(pred_logits, dim=1)\n",
        "            test_accuracy += (preds == y_test).sum().item()\n",
        "            loss = loss_fn(pred_logits, y_test).item()\n",
        "            test_loss += loss\n",
        "\n",
        "            if best_loss > loss:\n",
        "                best_loss = loss\n",
        "                torch.save(model_path / model_name)\n",
        "\n",
        "    train_loss /= len(train_dataset)\n",
        "    test_loss /= len(test_dataset)\n",
        "    train_accuracy /= len(train_dataset)\n",
        "    test_accuracy /= len(test_dataset)\n",
        "\n",
        "    train_loss_hist.append(train_loss)\n",
        "    test_loss_hist.append(test_loss)\n",
        "    train_accuracy_hist.append(train_accuracy)\n",
        "    test_accuracy_hist.append(test_accuracy)\n",
        "\n",
        "    print(f\"epoch: {epoch+1} | train loss: {train_loss:.4f} | train accuracy: {train_accuracy * 100:.4f} | test loss: {test_loss:.4f} test accuracy: {test_accuracy * 100:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
